* Utils
** Unary Matrix Operators
#+begin_src python :tangle metrics_miscellany/utils.py
import numpy as np
from scipy import sparse
import pandas as pd

def inv(A):
    """Inverse of square pandas DataFrame."""
    if np.isscalar(A): A = pd.DataFrame(np.array([[A]]))

    B = np.linalg.inv(A)
    return pd.DataFrame(B,columns=A.columns,index=A.index)

def pinv(A):
    """Moore-Penrose pseudo-inverse of A.
    """
    if np.isscalar(A): A = pd.DataFrame(np.array([[A]]))

    B = np.linalg.pinv(A)
    return pd.DataFrame(B,columns=A.index,index=A.columns)


def diag(X,sparse=True):

    try:
        assert X.shape[0] == X.shape[1]
        d = pd.Series(np.diag(X),index=X.index)
    except IndexError: # X is a series?
        if sparse:
            # We can wind up blowing ram if not careful...
            d = sparse.diags(X.values)
            d = pd.DataFrame.sparse.from_spmatrix(d,index=X.index,columns=X.index)
        else:
            raise NotImplementedError
    except AttributeError: # Not a pandas object?
        d = np.diag(X)

    return d


#+end_src
** Binary Matrix Products
#+begin_src python :tangle metrics_miscellany/utils.py
def outer(S,T):
    """Outer product of two series (vectors) S & T.
    """
    return pd.DataFrame(np.outer(S,T),index=S.index,columns=T.index)

def matrix_product(X,Y):
    """Compute matrix product X@Y, allowing for possibility of missing data."""
    assert all(X.columns==Y.index), "Columns and Indices don't match."

    X = pd.DataFrame(X).fillna(0)

    if len(Y.shape)==1 or Y.shape[1]==1:
        Y = pd.Series(Y.squeeze()).fillna(0)
    else:
        Y = pd.DataFrame(Y).fillna(0)

    return np.matmul(X,Y)

def self_inner(X,min_obs=None):
    """Compute inner product X.T@X, allowing for possibility of missing data."""
    n,m=X.shape

    if n<m:
        axis=1
        N=m
    else:
        axis=0
        N=n

    xbar=X.mean(axis=axis)

    if axis:
        C=(N-1)*X.T.cov(min_periods=min_obs)
    else:
        C=(N-1)*X.cov(min_periods=min_obs)

    return C + N*np.outer(xbar,xbar)

def kron(A,B,sparse=False):
    if sparse: raise NotImplementedError

    a = np.kron(A,B)
    return pd.DataFrame(a,columns=pd.MultiIndex.from_product([A.columns.tolist(), B.columns.tolist()]),
                        index=pd.MultiIndex.from_product([A.index.tolist(), B.index.tolist()]))

#+end_src
*** Binary Operation Tests
#+begin_src python :tangle metrics_miscellany/test/test_binary_ops.py
import metrics_miscellany.datamat as dm
import pandas as pd
import numpy as np

def test_matmul(A,B):
    C = A@B
    assert isinstance(C,type(A))

    return C

def test_matmul_matvec(A,b):
    C = A@b
    assert isinstance(C,type(b))

    return C

if __name__=='__main__':
    A = np.array([[1,2],[3,4]])
    B = np.array([[1,1]]).T
    Cnp = test_matmul(A,B)

    A = pd.DataFrame([[1,2],[3,4]])
    B = pd.DataFrame([[1,1]]).T
    Cpd = test_matmul(A,B)

    A = dm.DataMat([[1,2],[3,4]])
    B = dm.DataMat([[1,1]]).T
    Cdm = test_matmul(A,B)

    b = dm.DataVec([1,1])
    c = test_matmul_matvec(A,b)

#+end_src
** Matrix Decompositions
#+begin_src python :tangle metrics_miscellany/utils.py
def heteropca(C,r=1,max_its=50,tol=1e-3,verbose=False):
    """Estimate r factors and factor weights of covariance matrix C."""
    from scipy.spatial import procrustes

    N = C - np.diag(np.diag(C))

    ulast = np.zeros((N.shape[1],r))
    u = np.zeros((N.shape[1],r))
    u[0,0] = 1
    ulast[-1,0] = 1

    t = 0

    while procrustes(u,ulast)[-1] >tol and t<max_its:
        ulast = u

        u,s,vt = np.linalg.svd(N,full_matrices=False,hermitian=True)

        s = s[:r]
        u = u[:,:r]

        Ntilde = u[:,:r]@np.diag(s[:r])@vt[:r,:]

        N = N - np.diag(np.diag(N)) + np.diag(np.diag(Ntilde))

        t += 1

        if t==max_its:
            warnings.warn("Exceeded maximum iterations (%d)" % max_its)
        if verbose: print(f"Iteration {t}, u[0,:r]={u[0,:r]}.")

    return u,s

def svd_missing(A,max_rank=None,min_obs=None,heteroskedastic=False,verbose=False):
    """Singular Value Decomposition with missing values

    Returns matrices U,S,V.T, where A~=U*S*V.T.

    Inputs:
        - A :: matrix or pd.DataFrame, with NaNs for missing data.

        - max_rank :: Truncates the rank of the representation.  Note
                      that this impacts which rows of V will be
                      computed; each row must have at least max_rank
                      non-missing values.  If not supplied rank may be
                      truncated using the Kaiser criterion.

        - min_obs :: Smallest number of non-missing observations for a
                     row of U to be computed.

        - heteroskedastic :: If true, use the "heteroPCA" algorithm
                       developed by Zhang-Cai-Wu (2018) which offers a
                       correction to the svd in the case of
                       heteroskedastic errors.  If supplied as a pair,
                       heteroskedastic[0] gives a maximum number of
                       iterations, while heteroskedastic[1] gives a
                       tolerance for convergence of the algorithm.

    Ethan Ligon                                        September 2021

    """
    # Defaults; modify by passing a tuple to heteroskedastic argument.
    max_its=50
    tol = 1e-3

    P = self_inner(A,min_obs=min_obs) # P = A.T@A

    sigmas,v=np.linalg.eigh(P)

    order=np.argsort(-sigmas)
    sigmas=sigmas[order]

    # Truncate rank of representation using Kaiser criterion (positive eigenvalues)
    v=v[:,order]
    v=v[:,sigmas>0]
    s=np.sqrt(sigmas[sigmas>0])

    if max_rank is not None and len(s) > max_rank:
        v=v[:,:max_rank]
        s=s[:max_rank]

    r=len(s)

    if heteroskedastic: # Interpret tuple
        try:
            max_its,tol = heteroskedastic
        except TypeError:
            pass
        Pbar = P.mean()
        v,s = heteropca(P-Pbar,r=r,max_its=max_its,tol=tol,verbose=verbose)

    if A.shape[0]==A.shape[1]: # Symmetric; v=u
        return v,s,v.T
    else:
        vs=v@np.diag(s)

        u=np.zeros((A.shape[0],len(s)))
        for j in range(A.shape[0]):
            a=A.iloc[j,:].values.reshape((-1,1))
            x=np.nonzero(~np.isnan(a))[0] # non-missing elements of vector a
            if len(x)>=r:
                u[j,:]=(np.linalg.pinv(vs[x,:])@a[x]).reshape(-1)
            else:
                u[j,:]=np.nan

    s = pd.Series(s)
    u = pd.DataFrame(u,index=A.index)
    v = pd.DataFrame(v,index=A.columns)

    return u,s,v
#+end_src
** DataFrame Manipulations
#+begin_src python :tangle metrics_miscellany/utils.py
from cfe.df_utils import use_indices
from pandas import concat, get_dummies, MultiIndex

def dummies(df,cols,suffix=False):
    """From a dataframe df, construct an array of indicator (dummy) variables,
    with a column for every unique row df[cols]. Note that the list cols can
    include names of levels of multiindices.

    The optional argument =suffix=, if provided as a string, will append suffix
    to column names of dummy variables. If suffix=True, then the string '_d'
    will be appended.
    """
    idxcols = list(set(df.index.names).intersection(cols))
    colcols = list(set(cols).difference(idxcols))

    v = concat([use_indices(df,idxcols),df[colcols]],axis=1)

    v['tuple'] = list(zip(*[v[s] for s in idxcols+colcols]))

    v = get_dummies(v['tuple']).astype(int)

    if suffix==True:
        suffix = '_d'

    if suffix!=False and len(suffix)>0:
        columns = [tuple([str(c)+suffix for c in t]) for t in v.columns]
    else:
        columns = v.columns
        
    v.columns = MultiIndex.from_tuples(columns,names=idxcols+colcols)

    return v
#+end_src
* DataMat
#+begin_src python :tangle metrics_miscellany/datamat.py
import pandas as pd
import numpy as np
from metrics_miscellany.utils import matrix_product, diag
from metrics_miscellany.utils import inv as matrix_inv
from metrics_miscellany.utils import pinv as matrix_pinv
import metrics_miscellany.utils as utils
from functools import cached_property
from scipy import sparse as scipy_sparse

class DataVec(pd.Series):
    __pandas_priority__ = 5000

    def __init__(self, *args, **kwargs):
        """Create a DataVec.  

        Inherit from :meth: `pd.Series.__init__`.

        Additional Parameters
        ---------------------
        idxnames 
                (List of) name(s) for levels of index.
        """
        if 'idxnames' in kwargs.keys():
            idxnames = kwargs.pop('idxnames')
        else:
            idxnames = None

        super(DataMat, self).__init__(*args,**kwargs)

        if idxnames is not None:
            if isinstance(idxnames,str):
                self.index.name = idxnames
            else:
                self.index.names = idxnames

    @property
    def _constructor(self):
        return DataVec

    @property
    def _constructor_expanddim(self):
        return DataMat

    def dg(self,sparse=True):
        """Return"""
        if sparse:
            # We can wind up blowing ram if not careful...
            d = scipy_sparse.diags(self.values)
            return DataMat(pd.DataFrame.sparse.from_spmatrix(d,index=self.index,columns=self.index))
        else:
            return DataMat(np.diag(self.values),index=self.index,columns=self.index)

class DataMat(pd.DataFrame):
    __pandas_priority__ = 6000

    def __init__(self, *args, **kwargs):
        """Create a DataVec.  

        Inherit from :meth: `pd.DataFrame.__init__`.

        Additional Parameters
        ---------------------
        idxnames 
                (List of) name(s) for levels of index.
        colnames 
                (List of) name(s) for levels of columns.
        """
        if 'idxnames' in kwargs.keys():
            idxnames = kwargs.pop('idxnames')
        else:
            idxnames = None
        if 'colnames' in kwargs.keys():
            colnames = kwargs.pop('colnames')
        else:
            colnames = None

        super(DataMat, self).__init__(*args,**kwargs)

        if idxnames is not None:
            if isinstance(idxnames,str):
                self.index.name = idxnames
            else:
                self.index.names = idxnames
        if colnames is not None:
            if isinstance(colnames,str):
                self.columns.name = colnames
            else:
                self.columns.names = colnames

    @property
    def _constructor(self):
        return DataMat

    @property
    def _constructor_sliced(self):
        return DataVec

    # Unary operations
    @cached_property
    def inv(self):
        return matrix_inv(self)

    @property
    def dg(self):
        """Extract diagonal from square matrix.

        >>> dg([[1,2][3,4]])
        [1,4]
        """
        assert np.all(self.index==self.columns), "Should have columns same as index."
        return DataVec(np.diag(self.values),index=self.index)

    def pinv(self):
        """Moore-Penrose pseudo-inverse."""
        return matrix_pinv(self)

    def matmul(self,other):
        Y = matrix_product(self,other)
        return DataMat(Y)

    def kron(self,other,sparse=False):
        return DataMat(utils.kron(self,other,sparse=sparse))

    # Other transformations
    def dummies(self,cols,suffix=''):
        return utils.dummies(self,cols,suffix='')

    def concat(self,other,axis=0):
        return DataMat(utils.concat([self,other],axis=axis))


if __name__ == "__main__":
    import doctest
    doctest.testmod()
#+end_src

* Estimators
** Preliminaries
#+begin_src python :tangle metrics_miscellany/estimators.py
import statsmodels.api as sm
from statsmodels.stats import correlation_tools
import numpy as np
from numpy.linalg import lstsq
import warnings
import pandas as pd
from . import gmm
from . GMM_class import GMM
from . import utils
from .datamat import DataMat, DataVec
#+end_src
** OLS
#+begin_src python :tangle metrics_miscellany/estimators.py

def ols(X,y,cov_type='HC3',PSD_COV=False):
    """OLS estimator of b in y = Xb + u.

    Returns both estimate b as well as an estimate of Var(b).

    The estimator used for the covariance matrix depends on the
    optional argument =cov_type=.

    If optional flag PSD_COV is set, then an effort is made to ensure that
    the estimated covariance matrix is positive semi-definite.  If PSD_COV is
    set to a positive float, then this will be taken to be the smallest eigenvalue
    of the 'corrected' matrix.
    """
    n,k = X.shape

    est = sm.OLS(y,X).fit()
    b = pd.DataFrame({'Coefficients':est.params.values},index=X.columns)
    if cov_type=='HC3':
        V = est.cov_HC3
    elif cov_type=='OLS':
        XX = X.T@X
        if np.linalg.eigh(XX)[0].min()<0:
            XX = correlation_tools.cov_nearest(XX,method='nearest')
            warnings.warn("X'X not positive (semi-) definite.  Correcting!  Estimated variances should not be affected.")
        V = est.resid.var()*np.linalg.inv(XX)
    elif cov_type=='HC2':
        V = est.cov_HC2
    elif cov_type=='HC1':
        V = est.cov_HC1
    elif cov_type=='HC0':
        V = est.cov_HC0
    else:
        raise ValueError("Unknown type of covariance matrix.")

    if PSD_COV:
        if PSD_COV is True:
            PSD_COV = (b**2).min()
        s,U = np.linalg.eigh((V+V.T)/2)
        if s.min()<PSD_COV:
            oldV = V
            V = U@np.diag(np.maximum(s,PSD_COV))@U.T
            warnings.warn("Estimated covariance matrix not positive (semi-) definite.\nCorrecting! Norm of difference is %g." % np.linalg.norm(oldV-V))

    V = pd.DataFrame(V,index=X.columns,columns=X.columns)

    return b,V

#+end_src

*** OLS Tests
#+begin_src python :tangle metrics_miscellany/test/test_ols.py
import pandas as pd
from metrics_miscellany.estimators import ols
import numpy as np

def test_ols(N=500000,tol=1e-2):

    x = pd.DataFrame({'x':np.random.standard_normal((N,))})
    x['Constant'] = 1

    beta = pd.DataFrame({'Coefficients':[1,0]},index=['x','Constant'])

    u = pd.DataFrame(np.random.standard_normal((N,))/10)

    y = (x@beta).values + u.values
    b,V = ols(x,y)

    assert np.allclose(b,beta,atol=tol)

if __name__=='__main__':
    test_ols()

#+end_src
** Two-stage Least Squares
#+begin_src python :tangle metrics_miscellany/estimators.py
def tsls(X,y,Z,return_Omega=False):
    """
    Two-stage least squares estimator.
    """

    n,k = X.shape

    Qxz = X.T@Z/n

    zzinv = utils.inv(Z.T@Z/n)
    b = lstsq(Qxz@zzinv@Qxz.T,Qxz@zzinv@Z.T@y/n,rcond=None)[0]

    b = pd.Series(b.squeeze(),index=X.columns)

    # Cov matrix
    e = y.squeeze() - X@b

    #Omega = Z.T@(e**2).dg()@Z/n
    # Rather than forming even a sparse nxn matrix, just use element-by-element multiplication
    ZTe = Z.T.multiply(e)
    Omega = ZTe@ZTe.T/n

    #Omega = pd.DataFrame(e.var()*Z.T.values@Z.values/n,columns=Z.columns,index=Z.columns)

    if return_Omega:
        return b,Omega
    else:
        A = (Qxz@zzinv@Qxz.T).inv
        V = A@(Qxz@zzinv@Omega@zzinv@Qxz.T)@A.T/n
        return b,V

#+end_src
*** TSLS Tests
#+begin_src python :tangle metrics_miscellany/test/test_tsls.py
import pandas as pd
from metrics_miscellany.estimators import tsls, ols
from metrics_miscellany.datamat import DataMat, DataVec
import numpy as np

def test_tsls(N=500000,tol=1e-2):

    z = DataMat({'z':np.random.standard_normal((N,))})
    u = DataVec(np.random.standard_normal((N,)))
    x = DataMat({'x':z.squeeze() + u})

    x['Constant'] = 1
    z['Constant'] = 1

    beta = DataMat({'Coefficients':[1,0]},index=['x','Constant'])

    y = (x@beta).squeeze() + u

    #b,V = tsls(x,y,z)
    b,V = ols(x,y)

    assert np.allclose(b,beta.squeeze(),atol=tol)

if __name__=='__main__':
    test_tsls()

#+end_src

** Linear GMM
#+begin_src python :tangle metrics_miscellany/estimators.py
def linear_gmm(X,y,Z,W=None,return_Omega=False):
    """
    Linear GMM estimator.
    """

    if W is None: # Use 2sls to get initial estimate of W
        b1,Omega1 = tsls(X,y,Z,return_Omega=True)
        W = Omega1.inv
        return linear_gmm(X,y,Z,W=W)
    else:
        n,k = X.shape

        Qxz = X.T@Z/n

        b = lstsq(Qxz@W@Qxz.T,Qxz@W@Z.T@y/n,rcond=None)[0]

        b = pd.Series(b.squeeze(),index=X.columns)

        # Cov matrix
        e = y.squeeze() - X@b

        #Omega = Z.T@(e**2).dg()@Z/n
        # Rather than forming even a sparse nxn matrix, just use element-by-element multiplication
        ZTe = Z.T.multiply(e)
        Omega = ZTe@ZTe.T/n

        if return_Omega:
            return b,Omega
        else:
            Vb = (Qxz@Omega.inv@Qxz.T).inv/n
            return b,Vb

#+end_src
*** Linear GMM Tests
#+begin_src python :tangle metrics_miscellany/test/test_linear_gmm.py
import pandas as pd
from metrics_miscellany.estimators import linear_gmm
from metrics_miscellany.datamat import DataMat, DataVec
import numpy as np

def test_linear_gmm(N=500000,tol=1e-2):

    z = DataMat(np.random.standard_normal((N,2)),columns=['z1','z2'])
    u = DataVec(np.random.standard_normal((N,)))
    x = DataMat({'x':z.sum(axis=1) + u})

    x['Constant'] = 1
    z['Constant'] = 1

    beta = DataMat({'Coefficients':[1,0]},index=['x','Constant'])

    y = (x@beta).squeeze() + u

    b,V = linear_gmm(x,y,z)

    assert np.allclose(b,beta.squeeze(),atol=tol)

    return b,V

if __name__=='__main__':
    b,V = test_linear_gmm()

#+end_src

** Factor Regression
We're interested here in multivariate regressions of the form
\[
     Y = XB + F\Lambda + U
\]
where $Y$ and $U$ are $N\times k$, $X$ is $n\times \ell$, $B$ is $\ell\times k$, $F$ is $N\times r$, and \Lambda is $r\times k$.  Only $(Y,X)$ are observed; $F$ is a collection of latent "factors."  The identifying assumptions are that $U$ is orthogonal to $(X,F)$ and $\E F_iF_i^\T = I_r$.  [cite/t:@hansen20:econometrics] describes an iterative approach to estimation, which we implement below.
#+begin_src python :tangle metrics_miscellany/estimators.py
def factor_regression(Y,X,F=None,rank=1,tol=1e-3):

    if rank>1:
        raise NotImplementedError("Factor regression for rank>1 is not reliable.")

    N,k = Y.shape
    def ols(X,Y):
        N,k = Y.shape
        XX = utils.self_inner(X)/N
        XY = utils.matrix_product(X.T,Y)/N
        B = np.linalg.lstsq(XX,XY,rcond=None)[0]
        return pd.DataFrame(B,index=X.columns,columns=Y.columns)

    if F is None:
        B = ols(X,Y)
        F = 0
    else:
        parms = ols(pd.concat([X,F],axis=1),Y)
        L = parms.iloc[-rank:,:]
        B = parms.iloc[:-rank,:]

    lastF = F
    F,s,vt = utils.svd_missing(Y - utils.matrix_product(X,B),max_rank=rank)
    scale = F.std()
    F = F.multiply(1/scale)

    if np.linalg.norm(F-lastF)>tol:
        B,L,F = factor_regression(Y,X,F=F,rank=rank,tol=tol)

    return B,L,F


#+end_src
*** Factor Regression Test
#+begin_src python :tangle metrics_miscellany/test/test_factor_regression.py
import pandas as pd
from scipy import stats
from metrics_miscellany.estimators import factor_regression
from metrics_miscellany import utils
import numpy as np

def generate_multivariate_normal(N,k,V=None,colidx='a'):

    try:
        a = ord(colidx)
        labels = list(map(chr, range(a, a+k)))
    except TypeError:
        labels = range(colidx,colidx+k)

    if V is None:
        D = pd.DataFrame(np.random.randn(k,k),index=labels,columns=labels)
        V = D.T@D
    else:
        V = pd.DataFrame(V,index=labels,columns=labels)

    X = pd.DataFrame(stats.multivariate_normal(cov=V).rvs(N),columns=labels)

    return X

def main(N,k,l,r):

    U = generate_multivariate_normal(N,k,V=np.eye(k),colidx='A')/100

    X = generate_multivariate_normal(N,l)

    a = ord('a')
    A = ord('A')
    rlabels = list(map(chr, range(a, a+l)))
    clabels = list(map(chr, range(A, A+k)))
    B = pd.DataFrame(np.arange(1,l*k+1).reshape(l,k),index=rlabels,columns=clabels)

    F = generate_multivariate_normal(N,r,colidx=0)
    F = F - F.mean()
    scale = F.std()
    F = F.multiply(1/scale)

    L = pd.DataFrame(np.arange(1,k*r+1).reshape(r,k)/10,index=F.columns,columns=U.columns)
    L = L.multiply(scale,axis=0)

    Y = utils.matrix_product(X,B) + utils.matrix_product(F,L) + U

    return Y,X,F,B,L,U

def test_factor_regression(N=1000,k=10,l=2,r=1):
    Y,X,F0,B0,L0,U0 = main(N,k,l,r)
    X['Constant'] = 1

    B,L,F = factor_regression(Y,X,rank=r)

    assert np.linalg.norm(F0-F) < np.linalg.norm(F0)

    assert np.all(Y.var()>(Y-X@B).var())

    assert np.all((Y-X@B).var()>(Y-X@B-F@L).var())

    assert np.linalg.norm((B0-B).dropna())/np.linalg.norm(B0) < 0.01

if __name__ == '__main__':
    test_factor_regression(N=10000,r=1)
 #+end_src

* GMM
** Procedural interface for GMM estimator.
#+begin_src python :tangle metrics_miscellany/gmm.py
import numpy as np
from . import utils
from . import utils
matrix_product = utils.matrix_product
diag = utils.diag
inv = utils.inv

from scipy.optimize import minimize_scalar, minimize
from scipy.optimize import minimize as scipy_min
import pandas as pd

from IPython.core.debugger import Pdb

__version__ = "0.3.1"

######################################################
# Beginning of procedural version of gmm routines

def gN(b):
    """Averages of g_j(b).

    This is generic for data, to be passed to gj.
    """
    e = gj(b)

    gN.N,gN.k = e.shape
    gN.N = e.count()  # Allows for possibility of missing data
    # Check to see more obs. than moments.
    assert np.all(gN.N > gN.k), "More moments than observations"

    try:
        return e.mean(axis=0).reshape((-1,1))
    except AttributeError:
        return e.mean(axis=0)

def Omegahat(b):
    e = gj(b)

    # Recenter! We have Eu=0 under null.
    # Important to use this information.
    e = e - e.mean(axis=0)
    sqrtN = np.sqrt(e.count())

    e = e/sqrtN

    ete = matrix_product(e.T,e)

    return ete

def J(b,W):

    m = gN(b) # Sample moments @ b

    #Pdb().set_trace()

    # Scaling by diag(N) allows us to deal with missing values
    WN = pd.DataFrame(matrix_product(diag(gN.N),W))

    crit = (m.T@WN@m).squeeze()
    assert crit > 0

    return crit

def minimize(f,b_init=None):
    if b_init is None:
        return minimize_scalar(f).x
    else:
        return scipy_min(f,b_init).x

def one_step_gmm(W=None,b_init=None):

    if b_init is None:
        b_init = 0

    if W is None:
        e = gj(b_init)
        W = pd.DataFrame(np.eye(e.shape[1]),index=e.columns,columns=e.columns)

    assert np.linalg.matrix_rank(W)==W.shape[0]

    b = minimize(lambda b: J(b,W),b_init=b_init)

    return b, J(b,W)

def two_step_gmm(b_init=None):

    # First step uses identity weighting matrix
    b1 = one_step_gmm(b_init=b_init)[0]

    # Construct 2nd step weighting matrix using
    # first step estimate of beta
    W2 = utils.inv(Omegahat(b1))

    return one_step_gmm(W=W2,b_init=b1)

def continuously_updated_gmm(b_init=None):

    # First step uses identity weighting matrix
    W = lambda b: utils.inv(Omegahat(b))

    bhat = minimize(lambda b: J(b,utils.inv(Omegahat(b))),b_init=b_init)

    return bhat, J(bhat,W(bhat))


def Jay(b):

    W = lambda b: utils.inv(Omegahat(b))
    g = lambda b: gN(b)

    return g(b).T@W(b)@g(b)

def Vb(b):
    """Covariance of estimator of b.

    Note that one must supply gmm.dgN, the average gradient of gmm.gj at b.
    """
    Q = dgN(b)
    W = pd.DataFrame(matrix_product(diag(gN.N),Omegahat(b)))

    return utils.inv(Q.T@utils.inv(W)@Q)

def print_version():
    print(__version__)

# End of procedural version of gmm routines
######################################################
#+end_src
*** GMM Test
#+begin_src python :tangle metrics_miscellany/test/test_gmm.py
import pandas as pd
import numpy as np
from metrics_miscellany.estimators import gmm
from numpy.linalg import inv
from scipy.stats import distributions as iid

def dgp(N,beta,gamma,sigma_u,VXZ):
    """Generate a tuple of (y,X,Z).

    Satisfies model:
        y = X@beta + u
        E Z'u = 0
        Var(u) = sigma^2
        Cov(X,u) = gamma*sigma_u^2
        Var([X,Z]|u) = VXZ
        u,X,Z mean zero, Gaussian

    Each element of the tuple is an array of N observations.

    Inputs include
    - beta :: the coefficient of interest
    - gamma :: linear effect of disturbance on X
    - sigma_u :: Variance of disturbance
    - VXZ :: Var([X,Z]|u)
    """

    u = pd.Series(iid.norm.rvs(size=(N,))*sigma_u)

    # "Square root" of VXZ via eigendecomposition
    lbda,v = np.linalg.eig(VXZ)
    SXZ = v@np.diag(np.sqrt(lbda))

    # Generate normal random variates [X*,Z]
    XZ = pd.DataFrame(iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T)

    # But X is endogenous...
    X = XZ.loc[:,0].add(gamma*u,axis=0)
    Z = XZ.loc[:,1:]

    # Calculate y
    y = X*beta + u

    return y,X,Z

def test_gmm(N=10000):

    ## Play with us!
    beta = 1     # "Coefficient of interest"
    gamma = 1    # Governs effect of u on X
    sigma_u = 1  # Note assumption of homoskedasticity
    ## Play with us!

    # Let Z have order ell, and X order 1, with Var([X,Z]|u)=VXZ

    ell = 4 # Play with me too!

    # Arbitrary (but deterministic) choice for VXZ = [VX Cov(X,Z);
    #                                                 Cov(Z,X) VZ]
    # Pinned down by choice of a matrix A...
    A = np.sqrt(1/np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1))

    ## Below here we're less playful.

    # Now Var([X,Z]|u) is constructed so guaranteed pos. def.
    VXZ = A.T@A

    Q = -VXZ[1:,[0]]  # -EZX', or generally Edgj/db'

    # Gimme some truth:
    truth = (beta,gamma,sigma_u,VXZ)

    ## But play with Omega if you want to introduce heteroskedascity
    Omega = (sigma_u**2)*VXZ[1:,1:] # E(Zu)(u'Z')

    # Asymptotic variance of optimally weighted GMM estimator:
    AVar_b = inv(Q.T@inv(Omega)@Q)

    data = dgp(N,*truth)

    def gj(b):
        y,X,Z = data
        e = (y.squeeze()-b*X.squeeze())

        Ze = Z.multiply(e,axis=0)

        return Ze

    def dgN(b):
        y,X,Z = data
        return Z.T@X

    gmm.gj = gj
    gmm.dgN = dgN

    b,J = gmm.two_step_gmm()

    assert (b-beta)**2 < 0.01, f"Estimate {b} outside tolerance."

    print(b,J)
    print(gmm.Vb(b))

    return J

if __name__ == '__main__':
    J = []
    for i in range(1000): J.append(test_gmm())

#+end_src
** GMM Class
#+begin_src python :tangle metrics_miscellany/GMM_class.py
from . import gmm
import numpy as np

class GMM(object):

    def __init__(self,gj,data,B,W=None):
        """GMM problem for restrictions E(gj(b0))=0, estimated using data with b0 in R^k.

           - If supplied B is a positive integer k, then
             space taken to be R^k.
           - If supplied B is a k-vector, then
             parameter space taken to be R^k with B a possible
             starting value for optimization.
        """
        self.gj = gj
        gmm.gj = gj  # Overwrite member of gmm module
        self.data = data

        self.W = W

        self.b = None

        try:
            self.k = len(B)
            self.b_init = np.array(B)
        except TypeError:
            self.k = B
            self.b_init = np.zeros(self.k)

        self.ell = gj(self.b_init,self.data).shape[1]

        if type(data) is tuple:
            self.N = data[0].shape[0]
        else:
            self.N = data.shape[0]

        self.minimize = gmm.minimize

    def gN(self,b):
        """Averages of g_j(b).

        This is generic for data, to be passed to gj.
        """
        return gmm.gN(b,self.data)

    def Omegahat(self,b):

        return gmm.Omegahat(b,self.data)

    def J(self,b,W):

        return gmm.J(b,W,self.data)

    def one_step_gmm(self,W=None,b_init=None):

        self.b = gmm.one_step_gmm(self.data,W,b_init=self.b_init)[0]

        return self.b

    def two_step_gmm(self):

        self.b = gmm.two_step_gmm(self.data,b_init=self.b_init)[0]
        self.W = np.linalg.inv(self.Omegahat(self.b))

        return self.b

    def continuously_updated_gmm(self):

        est = gmm.continuously_updated_gmm(self.data,b_init=self.b_init)[0]
        self.b = est
        self.W = np.linalg.inv(self.Omegahat(self.b))

        return self.b



if __name__=='__main__':
    #foo = GMM(gmm.gj,
    pass

#+end_src

* Hypothesis Tests
** Chi square tests
#+begin_src python :tangle metrics_miscellany/tests.py
from metrics_miscellany import utils
from scipy import stats
import pandas as pd
import numpy as np

def chi2_test(b,V,var_selection=None,R=None,TEST=False):
    """Construct chi2 test of R'b = 0.

    If R is None then test is b = 0.

    If one wishes to test a hypothesis regarding only a subset of elements of b,
    this subset can be chosen by specifying var_selection as either a query string
    or as a list.
    """

    if var_selection is not None:
        if type(var_selection) is str:
            myb = b.query(var_selection)
        elif type(var_selection) is list:
            myb = b.loc[var_selection]
        else:
            raise(ValueError,"var_selection should be a query string of list of variable names")
    else:
        myb = b


    # Drop parts of matrix not involved in test
    myV = V.reindex(myb.index,axis=0).reindex(myb.index,axis=1)

    myV = utils.cov_nearest(myV,threshold=1e-10)

    if R is not None:
        myV = R.T@myV@R
        myb = R.T@b
        if np.isscalar(myV):
            myV = np.array([[myV]])
            myb = np.array([[myb]])

    if TEST: # Generate values of my that satisfy Var(myb)=Vb and Emyb=0
        myb = myb*0 + stats.multivariate_normal(cov=((1e0)*np.eye(myV.shape[0]) + myV)).rvs().reshape((-1,1))

    # "Invert"...

    L = np.linalg.cholesky(myV)
    y = np.linalg.solve(L.T,myb)

    chi2 = y.T@y

    y = pd.Series(y.squeeze(),index=myb.index)

    return chi2,1-stats.distributions.chi2.cdf(chi2,df=len(myb))


#+end_src

*** Test of chi2_test
#+begin_src python :tangle metrics_miscellany/test/test_chi2_test.py
import pandas as pd
from scipy import stats
from metrics_miscellany import tests
import numpy as np

def main():

    labels = ['a','b']
    D = pd.DataFrame([[2,1],[2,2]],index=labels,columns=labels)
    D.index.name = 'Variable'
    D.columns.name = 'Variable'

    V = D.T@D

    b = pd.DataFrame(stats.multivariate_normal(cov=V).rvs(),index=labels)
    b.index.name = 'Variable'

    return tests.chi2_test(b,V,"Variable in ['a']")

def test_chi2():
    p = []
    m = 1000
    for i in range(m):
        p.append(main()[1])

    p = pd.Series([x[0][0] for x in p]).squeeze()

    X = np.linspace(.05,.95,10)
    assert np.linalg.norm(p.quantile(X) - X)/len(X) < 1e-1

if __name__ == '__main__':
    test_chi2()
 #+end_src
** Skillings-Mack Test (Generalization of Friedman Test)
This implements a version of the test proposed in [cite/t:@skillings-mack81], which generalizes the Friedman rank test to the case in which data is incomplete.  Because the Friedman test is a special case, we also create a =friedman= test.
#+begin_src python :tangle metrics_miscellany/tests.py
def skillings_mack(df,bootstrap=False):
    """
    Non-parametric test of correlation across columns of df.

    Algorithm from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2761045/
    """
    def construct_statistic(R,kay):
        """
        Once we have ranks, construct SM statistic
        """
        # Fill missing ranks with (k_i+1)/2
        R = R.where(~np.isnan(R),(kay+1)/2,axis=1)

        # Construct adjusted observation matrix
        A = R.subtract((kay.values+1)/2,axis=1)@np.sqrt(12/(kay.values+1))

        # Count of observations in both columns k and l
        O = ~np.isnan(X)+0.

        Sigma = np.eye(O.shape[0]) - O@O.T

        # Delete diagonal
        Sigma = Sigma - np.diag(np.diag(Sigma))

        # Add minus column sums to diagonal
        Sigma = Sigma - np.diag(Sigma.sum())

        return A.T@np.linalg.pinv(Sigma)@A

    # Drop any rows with only one column
    X = df.loc[df.count(axis=1)>0]

    n,k = X.shape

    # Counts of obs per row ("treatments")
    kay = X.count(axis=0)

    # Counts of obs per column ("blocks")
    en = X.count(axis=1)

    R = X.rank(axis=0)

    SM = construct_statistic(R,kay)

    if not bootstrap:
        p = 1-stats.distributions.chi2.cdf(SM,df=n-1)
    else:
        if bootstrap == True:
            tol = 1e-03
        else:
            tol = bootstrap

        SE = 0
        lastSE = np.inf
        its = 0
        sms = []
        while (its < 30) or (np.abs(SE-lastSE) > tol):
            lastSE = SE
            scrambled = pd.DataFrame(np.apply_along_axis(np.random.permutation,axis=0,arr=R.values),
                                     index=R.index,columns=R.columns)
           
            sms.append(construct_statistic(scrambled,kay))
            SE = np.std(sms)
            its += 1
        p = np.mean(sms>SM)

    return SM,p

friedman = skillings_mack
#+end_src
*** Test of Skillings Mack
#+begin_src python :tangle metrics_miscellany/test/test_skillings_mack.py
import pandas as pd
import numpy as np
from metrics_miscellany import tests

def test_sm_against_R():
    """This is an example given in https://cran.r-project.org/web/packages/Skillings.Mack/Skillings.Mack.pdf
    """
    X = pd.DataFrame([[3.2, 3.1, 4.3, 3.5, 3.6, 4.5, np.nan, 4.3, 3.5],
                      [4.1, 3.9, 3.5, 3.6, 4.2, 4.7, 4.2, 4.6, np.nan],
                      [3.8, 3.4, 4.6, 3.9, 3.7, 3.7, 3.4, 4.4, 3.7],
                      [4.2, 4.,  4.8, 4., 3.9, np.nan, np.nan, 4.9, 3.9]])

    # This value of SM statistic matches that from R Skill.Mack routine
    assert np.allclose(tests.skillings_mack(X)[0],15.493)

def test_sm_type1():
    # Now try a random matrix
    ps = pd.Series([tests.skillings_mack(pd.DataFrame(np.random.rand(100,10)),bootstrap=0.02)[1] for i in range(100)])

    # p values should be distributed uniformly, with mean of 1/2
    tstat = (ps.mean()-1/2)/ps.std()

    assert np.abs(tstat)<2

if __name__=='__main__':
    test_sm_against_R()
    test_sm_type1()
#+end_src
** Randomization Inference
   Suppose we want to estimate a linear regression
   \[
       y = \alpha + X\beta + W\gamma + u.
   \]

   We obtain estimates $(b,V_b)$ of the coefficients $\beta$ and
   corresponding covariance matrix.  We want to be able to conduct a
   test of the hypothesis $R'\beta=0$.

   The idea here is to use resampling of just the variables $X$
   without replacement as a way of drawing inferences regarding
   \beta.  In particular, we randomly permute the rows of $X$,
   creating a new variable $P$, and estimate
   \[
       y = \alpha + P\delta + W\gamma + u,
   \]
   yielding estimates $(d,V_d)$ for the coefficients $\delta$ and the
   covariance matrix of these estimates.

   Note that $R'\E d = 0$ by construction, for any set of linear
   restructions $R$.  The linear restrictions themselves suggest a
   $\chi^2$ test; denote this statistic by $T(R,d,V)$.  We repeat the
   permute-estimate-test cycle many times.  Then the proportion of
   times that the test statistic associated with the test of
   $$R'(\beta-\hat\delta)>0$ gives us a \(p\)-value associated with a
   test of the null hypothesis that $\beta>c$.  A two-sided test can
   be constructed from the absolute difference in absolute values;
   i.e., $|\beta - \delta|>c$.

#+begin_src python :tangle metrics_miscellany/tests.py
import pandas as pd
import numpy as np
from metrics_miscellany.estimators import ols

def randomization_inference(vars,X,y,permute_levels=None,R=None,tol=1e-3,VERBOSE=False,return_draws=False):
    """
    Return p-values associated with hypothesis that coefficients
    associated with vars are jointly equal to zero.

    Ethan Ligon                                       June 2021
    """

    assert np.all([v in X.columns for v in vars]), "vars must correspond to columns of X."

    b,V = ols(X,y)

    beta = b.squeeze()[vars]
    chi2 = chi2_test(beta,V,R=R)[0]

    last = np.inf
    p = 0
    i = 0
    Chi2 = []
    while (np.linalg.norm(p-last)>tol) or (i < 30):
        last = p
        if permute_levels is None:
            P= pd.DataFrame(np.random.permutation(X.loc[:,vars]),index=X.index,columns=vars)
        else:
            levels = X.index.names
            fixed = X.index.names.difference(permute_levels)
            P = pd.DataFrame(X.loc[:,vars].unstack(fixed).sample(frac=1).stack(fixed).values,index=X.index,columns=vars)

        myX = pd.concat([X.loc[:,X.columns.difference(vars)],P],axis=1)
        b,V = ols(myX,y)
        Chi2.append(chi2_test(b.squeeze()[vars],V,R=R)[0])
        p = (chi2<Chi2[-1])/(i+1) + last*i/(i+1)
        i += 1
        if VERBOSE: print("Latest chi2 (randomized,actual,p): (%6.2f,%6.2f,%6.4f)" % (Chi2[-1],chi2,p))

    if return_draws:
        return p,pd.Series(Chi2)
    else:
        return p

#+end_src

*** Test of randomization inference
#+begin_src ipython :tangle metrics_miscellany/test/test_randomization_inference.py
import pandas as pd
import scipy.stats.distributions as dists
from metrics_miscellany import estimators, tests
import matplotlib.pyplot as plt

n=1000
p = 0.5
# Generate contextual variables; probability of being female is p
C = pd.DataFrame({'Female':dists.binom(1,p).rvs(size=n)})
C['Male'] = 1-C

delta = pd.Series({"Female":1.,"Male":0.5})

T1 = pd.Series(dists.norm.rvs(size=n),name='Treatment1')
T2 = pd.Series(dists.norm.rvs(size=n),name='Treatment2')

# Interactions:
TC = C.multiply(T1,axis=0)
TC.columns = ['TxFemale','TxMale']

# Construct RHS matrix
X = pd.concat([T1,T2,C,TC],axis=1).iloc[:,:-1]
dC = C@delta

# Generate outcome y with *no* treatment effect, to look for Type I errors
epsilon= pd.Series(dists.norm.rvs(size=n),name='epsilon')

Y = dC + epsilon
Y.name = 'outcome'

p_i = tests.randomization_inference(['Treatment1'],X,Y,VERBOSE=False)

# Generate outcome y with uniform treatment effect, to look for Type II errors
epsilon= pd.Series(dists.norm.rvs(size=n),name='epsilon')

Y = T1 + T2 + dC + epsilon
Y.name = 'outcome'

p_ii = tests.randomization_inference(['Treatment1'],X,Y,VERBOSE=False)

# Test Treatment1 == Treatment2
R = pd.DataFrame({'Coefficients':[1,-1]},index=['Treatment1','Treatment2'])

p_iii = tests.randomization_inference(['Treatment1','Treatment2'],X.drop('TxFemale',axis=1),Y,R=R,VERBOSE=True)

#+end_src

** Maunchy test of sphericity
This test asks whether, given a sample covariance matrix $S$, one can
reject the hypothesis that the population covariance matrix
$\Sigma=\sigma I$; i.e., whether the random vector with variance
matrix $\Sigma$ has a spherical distribution or not (note that the
test is obtained under the assumption that the random vectors are
normally distributed), and is due to Maunchy (1940)[fn:: See
cite:muirhead82 p. 334.].
#+begin_src python :tangle metrics_miscellany/tests.py
import numpy as np
from scipy.stats.distributions import chi2

def maunchy(C,N):
    """Given a sample covariance matrix C estimating using N observations,
       return p-value associated with test of whether the population
       covariance matrix is proportional to the identity matrix.
    """

    raise NotImplementedError

    m = C.shape[0]

    V = np.linalg.det(C)/((np.trace(C)/m)**m)

    rho = 1 - (2*m**2 + m + 2)/(6*m*(N-1))

    w2 = (m-1)*(m-2)*(m+2)*(2*m**3 + 6*m**2 + 3*m + 2)/(288*(m**2) * ((N-1)**2) * rho**2)

    gamma = (((N-1)*rho)**2)*w2

    x2 = -2*(N-1)*rho*np.log(V)  # Chi-squared statistic

    df = (m+2)*(m-1)/2

    px2 = chi2.cdf(x2,df)

    p = px2 + gamma/(((N-1)*rho)**2) * (chi2.cdf(x2,df+4) - px2)

    return x2,1 - px2
#+end_src
**** COMMENT Test of Maunchy
#+begin_src python :tangle metrics_miscellany/test/test_maunchy.py
import numpy as np
from metrics_miscellany import tests
import scipy.stats.distributions as iid
import matplotlib.pyplot as plt

N = 300
k = 10

Chi2 = []
P = []
for m in range(1000):
    X = iid.norm.rvs(size=(N,k))

    C = np.cov(X,rowvar=False)
    x,p = tests.maunchy(C,N)
    Chi2.append(x)
    P.append(p)

df = (k+2)*(k-1)/2


range = np.linspace(np.min(Chi2),np.max(Chi2),500)

fig,ax = plt.subplots()
ax.plot(range,[iid.chi2.pdf(x,df) for x in range])[0]
ax.hist(Chi2,bins=int(np.ceil(np.sqrt(len(Chi2)))),density=True)
#print(
#+end_src
** Inference on eigenvalues
Suppose we wish to test whether a covariance matrix has a structure $\Sigma =
\Lambda\Lambda^\T + \lambda I$, where $\Lambda$ is rank $r$.  This
structure is often assumed in exact factor models, for example.
cite:srivastava-khastri79 (\S 9.5) suggest a simple likelihood ratio test,
implemented here.
#+begin_src python :tangle metrics_miscellany/tests.py
import numpy as np
from scipy.stats.distributions import chi2

def kr79(C,q,N):
    """Given a sample mxm covariance matrix C estimating using N observations,
       return p-value associated with test of whether the population
       covariance matrix has last q eigenvalues equal or not, where q+k=m.
    """

    m = C.shape[0]

    l = np.linalg.eigvalsh(C)  # eigenvalues in *ascending* order

    Q = (np.prod(l[:q])/(np.mean(l[:q])**q))**(N/2) # LR test statistic

    x2 = -2*np.log(Q)  # Chi-squared statistic

    df = (q-1)*(q+2)/2

    px2 = chi2.cdf(x2,df)

    #p = px2 + gamma/(((N-1)*rho)**2) * (chi2.cdf(x2,df+4) - px2)

    return x2,1 - px2
#+end_src
**** Test of kr79
#+begin_src python :tangle metrics_miscellany/test/test_kr79.py
import numpy as np
from metrics_miscellany import tests
import scipy.stats.distributions as iid
from scipy import stats
import matplotlib.pyplot as plt

N = 3000
m = 10
r = 3
q = m - r

# Build covariance matrix for "systematic" variation
Sigma = iid.norm.rvs(size=(m,r))
Sigma = Sigma@Sigma.T   # Positive definite

l,v = np.linalg.eigh(Sigma)
l = np.maximum(l,0)

Ssqrt = v@np.diag(np.sqrt(l))@v

assert np.allclose(Sigma,Ssqrt@Ssqrt.T)

# Build covariance matrix for errors.
## Use (something proportional to) identity for tests of size, otherwise tests of power.
#Psi = 10.0*np.eye(m)  + np.diag(range(1,m+1))/(m**3)
Psi = np.eye(m)
Psi = Psi@Psi.T   # Positive definite

l,v = np.linalg.eigh(Psi)
l = np.maximum(l,0) 

Psisqrt = v@np.diag(np.sqrt(l))@v

assert np.allclose(Psi,Psisqrt@Psisqrt.T)


Chi2 = []
P = []
for s in range(1000):
    X = iid.norm.rvs(size=(N,m))@Ssqrt.T
    e = iid.norm.rvs(size=(N,m))@Psisqrt.T

    C = np.cov(X + e,rowvar=False)
    x,p = tests.kr79(C,q,N)
    Chi2.append(x)
    P.append(p)

df = (q+2)*(q-1)/2

xrange = np.linspace(np.min(Chi2),np.max(Chi2),500)

fig,ax = plt.subplots()
ax.hist(Chi2,bins=int(np.ceil(np.sqrt(len(Chi2)))),density=True)
ax.plot(xrange,[iid.chi2.pdf(x,df) for x in xrange])

assert stats.kstest(P,stats.distributions.uniform.cdf).pvalue>0.01
#+end_src
* Utils
** Some QR Tricks
*** QR Decomposition
Begin with wrapping the numpy qr decomposition to return dataframes.
#+begin_src python  :tangle metrics_miscellany/utils.py
import numpy as np
import pandas as pd

def qr(X):
    """
    Pandas-friendly QR decomposition.
    """
    assert X.shape[0]>=X.shape[1]

    Q,R = np.linalg.qr(X)
    Q = pd.DataFrame(Q,index=X.index, columns=X.columns)
    R = pd.DataFrame(R,index=X.columns, columns=X.columns)

    return Q,R
#+end_src

*** Leverage
Now, use the fact that the leverage of different observations in $X$
are the sums of squares of rows of $Q$ in the $QR$ decomposition
#+begin_src python :tangle metrics_miscellany/utils.py

def leverage(X):
    """
    Return leverage of observations in X (the diagonals of the hat matrix).
    """

    Q = qr(X)[0]

    return (Q**2).sum(axis=1)
#+end_src

*** Hat factory

Now construct a factory that returns a function to put the "hat"
on y.  Though mathematically this looks like $X(X'X)^{-1}X'=QQ'$ in
practice we don't want to construct an $N\times N$ matrix like this,
as it's often too expensive.

#+begin_src python :tangle metrics_miscellany/utils.py
def hat_factory(X):
    """
    Return a function hat(y) that returns X(X'X)^{-1}X'y.

    This is the least squares prediction of y given X.

    We use the fact that  the hat matrix is equal to QQ',
    where Q comes from the QR decomposition of X.
    """
    Q = qr(X)[0]

    def hat(y):
        return Q@(Q.T@y)

    return hat
#+end_src


**  "Fixing" matrices that aren't quite positive definite
#+begin_src python :tangle metrics_miscellany/utils.py
from statsmodels.stats.correlation_tools import cov_nearest as _cov_nearest
import pandas as pd

def cov_nearest(V,threshold=1e-12):
    """
    Return a positive definite matrix which is "nearest" to the symmetric matrix V,
    with the smallest eigenvalue not less than threshold.
    """
    s,U = np.linalg.eigh((V+V.T)/2) # Eigenvalue decomposition of symmetric matrix

    s = np.maximum(s,threshold)

    return V*0 + U@np.diag(s)@U.T  # Trick preserves attributes of dataframe V
#+end_src
** Trimming
#+begin_src python :tangle metrics_miscellany/utils.py
import pandas as pd
import numpy as np

def trim(df,alpha):
    """Trim values below alpha quantile and above (1-alpha) quantile.

    This maps individual extreme elements of df to NaN.
    """
    xmin = df.quantile(alpha)
    xmax = df.quantile(1-alpha)
    return df.where((df>=xmin)*(df<=xmax),np.nan)
#+end_src

* Random
** Random permutations of dataframes
#+begin_src python :tangle metrics_miscellany/random.py
import numpy as np
import pandas as pd

def permutation(df,columns=None,permute_levels=None):
    """Randomly permute rows of df[columns] at permute_levels.
    """

    df = pd.DataFrame(df) # Make sure we have a DataFrame.

    if columns is None: columns = df.columns

    if permute_levels is None:
        P = pd.DataFrame(np.random.permutation(df.loc[:,columns]),index=df.index,columns=columns)
    else:
       fixed = df.index.names.difference(permute_levels)
       P = pd.DataFrame(df.loc[:,columns].unstack(fixed).sample(frac=1).stack(fixed).values,index=df.index,columns=columns)

    return P

#+end_src
*** Test permutation
#+begin_src python :tangle metrics_miscellany/test/test_permutation.py
import numpy as np
import pandas as pd
from metrics_miscellany import random

T = pd.Series(np.random.rand(10)>.5)

df = pd.DataFrame({'a':T,'b':T}).stack()
df = df + 0
df.index.names = ['i','t']

p = random.permutation(df,permute_levels=['i'])

assert np.all(p.unstack('t').std(axis=1)==0)

#+end_src
