* Estimators
** Preliminaries
#+begin_src python :tangle metrics_miscellany/estimators.py
import statsmodels.api as sm
from statsmodels.stats import correlation_tools
import numpy as np
from numpy.linalg import lstsq
import warnings
import pandas as pd
from . import gmm
from . GMM_class import GMM
from . import utils
#+end_src
** Utils
#+begin_src python :tangle metrics_miscellany/utils.py
import numpy as np
from scipy import sparse
import pandas as pd

def inv(A):
    """Inverse of square pandas DataFrame."""
    if np.isscalar(A): A = pd.DataFrame(np.array([[A]]))

    B = np.linalg.inv(A)
    return pd.DataFrame(B,columns=A.columns,index=A.index)

def matrix_product(X,Y):
    """Compute matrix product X@Y, allowing for possibility of missing data."""

    return X.fillna(0)@Y.fillna(0)

def diag(X):

    try:
        assert X.shape[0] == X.shape[1]
        d = pd.Series(np.diag(X),index=X.index)
    except IndexError: # X is a series?
        # We can wind up blowing ram if not careful...
        d = sparse.diags(X.values)
        d = pd.DataFrame.sparse.from_spmatrix(d,index=X.index,columns=X.index)
    except AttributeError: # Not a pandas object?
        d = np.diag(X)

    return d

def outer(S,T):
    """Outer product of two series (vectors) S & T.
    """
    return pd.DataFrame(np.outer(S,T),index=S.index,columns=T.index)
#+end_src
** OLS
#+begin_src python :tangle metrics_miscellany/estimators.py

def ols(X,y,cov_type='HC3',PSD_COV=False):
    """OLS estimator of b in y = Xb + u.

    Returns both estimate b as well as an estimate of Var(b).

    The estimator used for the covariance matrix depends on the
    optional argument =cov_type=.

    If optional flag PSD_COV is set, then an effort is made to ensure that
    the estimated covariance matrix is positive semi-definite.  If PSD_COV is
    set to a positive float, then this will be taken to be the smallest eigenvalue
    of the 'corrected' matrix.
    """
    n,k = X.shape

    est = sm.OLS(y,X).fit()
    b = pd.DataFrame({'Coefficients':est.params.values},index=X.columns)
    if cov_type=='HC3':
        V = est.cov_HC3
    elif cov_type=='OLS':
        XX = X.T@X
        if np.linalg.eigh(XX)[0].min()<0:
            XX = correlation_tools.cov_nearest(XX,method='nearest')
            warnings.warn("X'X not positive (semi-) definite.  Correcting!  Estimated variances should not be affected.")
        V = est.resid.var()*np.linalg.inv(XX)
    elif cov_type=='HC2':
        V = est.cov_HC2
    elif cov_type=='HC1':
        V = est.cov_HC1
    elif cov_type=='HC0':
        V = est.cov_HC0
    else:
        raise ValueError("Unknown type of covariance matrix.")

    if PSD_COV:
        if PSD_COV is True:
            PSD_COV = (b**2).min()
        s,U = np.linalg.eigh((V+V.T)/2)
        if s.min()<PSD_COV:
            oldV = V
            V = U@np.diag(np.maximum(s,PSD_COV))@U.T
            warnings.warn("Estimated covariance matrix not positive (semi-) definite.\nCorrecting! Norm of difference is %g." % np.linalg.norm(oldV-V))

    V = pd.DataFrame(V,index=X.columns,columns=X.columns)

    return b,V

#+end_src

*** OLS Tests
#+begin_src python :tangle metrics_miscellany/test/test_ols.py
import pandas as pd
from metrics_miscellany.estimators import ols
import numpy as np

def test_ols(N=500000,tol=1e-2):

    x = pd.DataFrame({'x':np.random.standard_normal((N,))})
    x['Constant'] = 1

    beta = pd.DataFrame({'Coefficients':[1,0]},index=['x','Constant'])

    u = pd.DataFrame(np.random.standard_normal((N,))/10)

    y = (x@beta).values + u.values

    b,V = ols(x,y)

    assert np.allclose(b,beta,atol=tol)

if __name__=='__main__':
    test_ols()

#+end_src
** Two-stage Least Squares
#+begin_src python :tangle metrics_miscellany/estimators.py
def tsls(X,y,Z):
    """
    Two-stage least squares estimator.
    """

    n,k = X.shape

    Qxz = X.T@Z/n

    zzinv = utils.inv(Z.T@Z/n)
    b = lstsq(Qxz@zzinv@Qxz.T,Qxz@zzinv@Z.T@y/n,rcond=None)[0]

    b = pd.Series(b.squeeze(),index=X.columns)

    # Cov matrix
    e = y.squeeze() - X@b

    Omega = pd.DataFrame(Z.T.values@np.diag(e**2)@Z.values/n,columns=Z.columns,index=Z.columns)
    #Omega = pd.DataFrame(e.var()*Z.T.values@Z.values/n,columns=Z.columns,index=Z.columns)


    A = utils.inv(Qxz@zzinv@Qxz.T)
    V = A@(Qxz@zzinv@Omega@zzinv@Qxz.T)@A.T/n

    return b,V

#+end_src
* GMM
** Procedural interface for GMM estimator.
#+begin_src python :tangle metrics_miscellany/gmm.py
import numpy as np
from . import utils
from . import utils
matrix_product = utils.matrix_product
diag = utils.diag
inv = utils.inv

from scipy.optimize import minimize_scalar, minimize
from scipy.optimize import minimize as scipy_min
import pandas as pd

from IPython.core.debugger import Pdb

__version__ = "0.3.1"

######################################################
# Beginning of procedural version of gmm routines

def gN(b):
    """Averages of g_j(b).

    This is generic for data, to be passed to gj.
    """
    e = gj(b)

    gN.N,gN.k = e.shape
    gN.N = e.count()  # Allows for possibility of missing data
    # Check to see more obs. than moments.
    assert np.all(gN.N > gN.k), "More moments than observations"

    try:
        return e.mean(axis=0).reshape((-1,1))
    except AttributeError:
        return e.mean(axis=0)

def Omegahat(b):
    e = gj(b)

    # Recenter! We have Eu=0 under null.
    # Important to use this information.
    e = e - e.mean(axis=0)
    sqrtN = np.sqrt(e.count())

    e = e/sqrtN

    ete = matrix_product(e.T,e)

    return ete

def J(b,W):

    m = gN(b) # Sample moments @ b

    #Pdb().set_trace()

    # Scaling by diag(N) allows us to deal with missing values
    WN = pd.DataFrame(matrix_product(diag(gN.N),W))

    crit = (m.T@WN@m).squeeze()
    assert crit > 0

    return crit

def minimize(f,b_init=None):
    if b_init is None:
        return minimize_scalar(f).x
    else:
        return scipy_min(f,b_init).x

def one_step_gmm(W=None,b_init=None):

    if b_init is None:
        b_init = 0

    if W is None:
        e = gj(b_init)
        W = pd.DataFrame(np.eye(e.shape[1]),index=e.columns,columns=e.columns)

    assert np.linalg.matrix_rank(W)==W.shape[0]

    b = minimize(lambda b: J(b,W),b_init=b_init)

    return b, J(b,W)

def two_step_gmm(b_init=None):

    # First step uses identity weighting matrix
    b1 = one_step_gmm(b_init=b_init)[0]

    # Construct 2nd step weighting matrix using
    # first step estimate of beta
    W2 = utils.inv(Omegahat(b1))

    return one_step_gmm(W=W2,b_init=b1)

def continuously_updated_gmm(b_init=None):

    # First step uses identity weighting matrix
    W = lambda b: utils.inv(Omegahat(b))

    bhat = minimize(lambda b: J(b,utils.inv(Omegahat(b))),b_init=b_init)

    return bhat, J(bhat,W(bhat))

def Jay(b):

    W = lambda b: utils.inv(Omegahat(b))
    g = lambda b: gN(b)

    return g(b).T@W(b)@g(b)

def Vb(b):
    """Covariance of estimator of b.

    Note that one must supply gmm.dgN, the average gradient of gmm.gj at b.
    """
    Q = dgN(b)
    W = pd.DataFrame(matrix_product(diag(gN.N),Omegahat(b)))

    return utils.inv(Q.T@utils.inv(W)@Q)

def print_version():
    print(__version__)

# End of procedural version of gmm routines
######################################################
#+end_src
*** GMM Test
#+begin_src python :tangle metrics_miscellany/test/test_gmm.py
import pandas as pd
import numpy as np
from metrics_miscellany.estimators import gmm
from numpy.linalg import inv
from scipy.stats import distributions as iid

def dgp(N,beta,gamma,sigma_u,VXZ):
    """Generate a tuple of (y,X,Z).

    Satisfies model:
        y = X@beta + u
        E Z'u = 0
        Var(u) = sigma^2
        Cov(X,u) = gamma*sigma_u^2
        Var([X,Z]|u) = VXZ
        u,X,Z mean zero, Gaussian

    Each element of the tuple is an array of N observations.

    Inputs include
    - beta :: the coefficient of interest
    - gamma :: linear effect of disturbance on X
    - sigma_u :: Variance of disturbance
    - VXZ :: Var([X,Z]|u)
    """

    u = pd.Series(iid.norm.rvs(size=(N,))*sigma_u)

    # "Square root" of VXZ via eigendecomposition
    lbda,v = np.linalg.eig(VXZ)
    SXZ = v@np.diag(np.sqrt(lbda))

    # Generate normal random variates [X*,Z]
    XZ = pd.DataFrame(iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T)

    # But X is endogenous...
    X = XZ.loc[:,0].add(gamma*u,axis=0)
    Z = XZ.loc[:,1:]

    # Calculate y
    y = X*beta + u

    return y,X,Z

def test_gmm(N=10000):

    ## Play with us!
    beta = 1     # "Coefficient of interest"
    gamma = 1    # Governs effect of u on X
    sigma_u = 1  # Note assumption of homoskedasticity
    ## Play with us!

    # Let Z have order ell, and X order 1, with Var([X,Z]|u)=VXZ

    ell = 4 # Play with me too!

    # Arbitrary (but deterministic) choice for VXZ = [VX Cov(X,Z);
    #                                                 Cov(Z,X) VZ]
    # Pinned down by choice of a matrix A...
    A = np.sqrt(1/np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1))

    ## Below here we're less playful.

    # Now Var([X,Z]|u) is constructed so guaranteed pos. def.
    VXZ = A.T@A

    Q = -VXZ[1:,[0]]  # -EZX', or generally Edgj/db'

    # Gimme some truth:
    truth = (beta,gamma,sigma_u,VXZ)

    ## But play with Omega if you want to introduce heteroskedascity
    Omega = (sigma_u**2)*VXZ[1:,1:] # E(Zu)(u'Z')

    # Asymptotic variance of optimally weighted GMM estimator:
    AVar_b = inv(Q.T@inv(Omega)@Q)

    data = dgp(N,*truth)

    def gj(b):
        y,X,Z = data
        e = (y.squeeze()-b*X.squeeze())

        Ze = Z.multiply(e,axis=0)

        return Ze

    def dgN(b):
        y,X,Z = data
        return Z.T@X

    gmm.gj = gj
    gmm.dgN = dgN

    b,J = gmm.two_step_gmm()

    assert (b-beta)**2 < 0.01, f"Estimate {b} outside tolerance."

    print(b,J)
    print(gmm.Vb(b))

    return J

if __name__ == '__main__':
    J = []
    for i in range(1000): J.append(test_gmm())

#+end_src
** GMM Class
#+begin_src python :tangle metrics_miscellany/GMM_class.py
from . import gmm
import numpy as np

class GMM(object):

    def __init__(self,gj,data,B,W=None):
        """GMM problem for restrictions E(gj(b0))=0, estimated using data with b0 in R^k.

           - If supplied B is a positive integer k, then
             space taken to be R^k.
           - If supplied B is a k-vector, then
             parameter space taken to be R^k with B a possible
             starting value for optimization.
        """
        self.gj = gj
        gmm.gj = gj  # Overwrite member of gmm module
        self.data = data

        self.W = W

        self.b = None

        try:
            self.k = len(B)
            self.b_init = np.array(B)
        except TypeError:
            self.k = B
            self.b_init = np.zeros(self.k)

        self.ell = gj(self.b_init,self.data).shape[1]

        if type(data) is tuple:
            self.N = data[0].shape[0]
        else:
            self.N = data.shape[0]

        self.minimize = gmm.minimize

    def gN(self,b):
        """Averages of g_j(b).

        This is generic for data, to be passed to gj.
        """
        return gmm.gN(b,self.data)

    def Omegahat(self,b):

        return gmm.Omegahat(b,self.data)

    def J(self,b,W):

        return gmm.J(b,W,self.data)

    def one_step_gmm(self,W=None,b_init=None):

        self.b = gmm.one_step_gmm(self.data,W,b_init=self.b_init)[0]

        return self.b

    def two_step_gmm(self):

        self.b = gmm.two_step_gmm(self.data,b_init=self.b_init)[0]
        self.W = np.linalg.inv(self.Omegahat(self.b))

        return self.b

    def continuously_updated_gmm(self):

        est = gmm.continuously_updated_gmm(self.data,b_init=self.b_init)[0]
        self.b = est
        self.W = np.linalg.inv(self.Omegahat(self.b))

        return self.b



if __name__=='__main__':
    #foo = GMM(gmm.gj,
    pass

#+end_src

* Hypothesis Tests
** Chi square tests
#+begin_src python :tangle metrics_miscellany/tests.py
from metrics_miscellany import utils
from scipy import stats
import pandas as pd
import numpy as np

def chi2_test(b,V,var_selection=None,R=None,TEST=False):
    """Construct chi2 test of R'b = 0.

    If R is None then test is b = 0.

    If one wishes to test a hypothesis regarding only a subset of elements of b,
    this subset can be chosen by specifying var_selection as either a query string
    or as a list.
    """

    if var_selection is not None:
        if type(var_selection) is str:
            myb = b.query(var_selection)
        elif type(var_selection) is list:
            myb = b.loc[var_selection]
        else:
            raise(ValueError,"var_selection should be a query string of list of variable names")
    else:
        myb = b


    # Drop parts of matrix not involved in test
    myV = V.reindex(myb.index,axis=0).reindex(myb.index,axis=1)

    myV = utils.cov_nearest(myV,threshold=1e-10)

    if R is not None:
        myV = R.T@myV@R
        myb = R.T@b
        if np.isscalar(myV):
            myV = np.array([[myV]])
            myb = np.array([[myb]])

    if TEST: # Generate values of my that satisfy Var(myb)=Vb and Emyb=0
        myb = myb*0 + stats.multivariate_normal(cov=((1e0)*np.eye(myV.shape[0]) + myV)).rvs().reshape((-1,1))

    # "Invert"...

    L = np.linalg.cholesky(myV)
    y = np.linalg.solve(L.T,myb)

    chi2 = y.T@y

    y = pd.Series(y.squeeze(),index=myb.index)

    return chi2,1-stats.distributions.chi2.cdf(chi2,df=len(myb))


#+end_src

*** Test of chi2_test
 #+begin_src python :tangle metrics_miscellany/test/test_chi2_test.py
import pandas as pd
from scipy import stats
from metrics_miscellany import tests
import numpy as np

def main():

    labels = ['a','b']
    D = pd.DataFrame([[2,1],[2,2]],index=labels,columns=labels)
    D.index.name = 'Variable'
    D.columns.name = 'Variable'

    V = D.T@D

    b = pd.DataFrame(stats.multivariate_normal(cov=V).rvs(),index=labels)
    b.index.name = 'Variable'

    return tests.chi2_test(b,V,"Variable in ['a']")

def test_chi2():
    p = []
    m = 1000
    for i in range(m):
        p.append(main()[1])

    p = pd.Series([x[0][0] for x in p]).squeeze()

    X = np.linspace(.05,.95,10)
    assert np.linalg.norm(p.quantile(X) - X)/len(X) < 1e-1

if __name__ == '__main__':
    test_chi2()
 #+end_src
** Skillings-Mack Test (Generalization of Friedman Test)
This implements a version of the test proposed in [cite/t:@skillings-mack81], which generalizes the Friedman rank test to the case in which data is incomplete.  Because the Friedman test is a special case, we also create a =friedman= test.
#+begin_src python :tangle metrics_miscellany/tests.py
def skillings_mack(df,bootstrap=False):
    """
    Non-parametric test of correlation across columns of df.

    Algorithm from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2761045/
    """
    def construct_statistic(R,kay):
        """
        Once we have ranks, construct SM statistic
        """
        # Fill missing ranks with (k_i+1)/2
        R = R.where(~np.isnan(R),(kay+1)/2,axis=1)

        # Construct adjusted observation matrix
        A = R.subtract((kay.values+1)/2,axis=1)@np.sqrt(12/(kay.values+1))

        # Count of observations in both columns k and l
        O = ~np.isnan(X)+0.

        Sigma = np.eye(O.shape[0]) - O@O.T

        # Delete diagonal
        Sigma = Sigma - np.diag(np.diag(Sigma))

        # Add minus column sums to diagonal
        Sigma = Sigma - np.diag(Sigma.sum())

        return A.T@np.linalg.pinv(Sigma)@A

    # Drop any rows with only one column
    X = df.loc[df.count(axis=1)>0]

    n,k = X.shape

    # Counts of obs per row ("treatments")
    kay = X.count(axis=0)

    # Counts of obs per column ("blocks")
    en = X.count(axis=1)

    R = X.rank(axis=0)

    SM = construct_statistic(R,kay)

    if not bootstrap:
        p = 1-stats.distributions.chi2.cdf(SM,df=n-1)
    else:
        if bootstrap == True:
            tol = 1e-03
        else:
            tol = bootstrap

        SE = 0
        lastSE = np.inf
        its = 0
        sms = []
        while (its < 30) or (np.abs(SE-lastSE) > tol):
            lastSE = SE
            scrambled = pd.DataFrame(np.apply_along_axis(np.random.permutation,axis=0,arr=R.values),
                                     index=R.index,columns=R.columns)
           
            sms.append(construct_statistic(scrambled,kay))
            SE = np.std(sms)
            its += 1
        p = np.mean(sms>SM)

    return SM,p

friedman = skillings_mack
#+end_src
*** Test of Skillings Mack
#+begin_src python :tangle metrics_miscellany/test/test_skillings_mack.py
import pandas as pd
import numpy as np
from metrics_miscellany import tests

def test_sm_against_R():
    """This is an example given in https://cran.r-project.org/web/packages/Skillings.Mack/Skillings.Mack.pdf
    """
    X = pd.DataFrame([[3.2, 3.1, 4.3, 3.5, 3.6, 4.5, np.nan, 4.3, 3.5],
                      [4.1, 3.9, 3.5, 3.6, 4.2, 4.7, 4.2, 4.6, np.nan],
                      [3.8, 3.4, 4.6, 3.9, 3.7, 3.7, 3.4, 4.4, 3.7],
                      [4.2, 4.,  4.8, 4., 3.9, np.nan, np.nan, 4.9, 3.9]])

    # This value of SM statistic matches that from R Skill.Mack routine
    assert np.allclose(tests.skillings_mack(X)[0],15.493)

def test_sm_type1():
    # Now try a random matrix
    ps = pd.Series([tests.skillings_mack(pd.DataFrame(np.random.rand(100,10)),bootstrap=0.02)[1] for i in range(100)])

    # p values should be distributed uniformly, with mean of 1/2
    tstat = (ps.mean()-1/2)/ps.std()

    assert np.abs(tstat)<2

if __name__=='__main__':
    test_sm_against_R()
    test_sm_type1()
#+end_src
** Randomization Inference
   Suppose we want to estimate a linear regression
   \[
       y = \alpha + X\beta + W\gamma + u.
   \]

   We obtain estimates $(b,V_b)$ of the coefficients $\beta$ and
   corresponding covariance matrix.  We want to be able to conduct a
   test of the hypothesis $R'\beta=0$.

   The idea here is to use resampling of just the variables $X$
   without replacement as a way of drawing inferences regarding
   \beta.  In particular, we randomly permute the rows of $X$,
   creating a new variable $P$, and estimate
   \[
       y = \alpha + P\delta + W\gamma + u,
   \]
   yielding estimates $(d,V_d)$ for the coefficients $\delta$ and the
   covariance matrix of these estimates.

   Note that $R'\E d = 0$ by construction, for any set of linear
   restructions $R$.  The linear restrictions themselves suggest a
   $\chi^2$ test; denote this statistic by $T(R,d,V)$.  We repeat the
   permute-estimate-test cycle many times.  Then the proportion of
   times that the test statistic associated with the test of
   $$R'(\beta-\hat\delta)>0$ gives us a \(p\)-value associated with a
   test of the null hypothesis that $\beta>c$.  A two-sided test can
   be constructed from the absolute difference in absolute values;
   i.e., $|\beta - \delta|>c$.

#+begin_src python :tangle metrics_miscellany/tests.py
import pandas as pd
import numpy as np
from metrics_miscellany.estimators import ols

def randomization_inference(vars,X,y,permute_levels=None,R=None,tol=1e-3,VERBOSE=False,return_draws=False):
    """
    Return p-values associated with hypothesis that coefficients
    associated with vars are jointly equal to zero.

    Ethan Ligon                                       June 2021
    """

    assert np.all([v in X.columns for v in vars]), "vars must correspond to columns of X."

    b,V = ols(X,y)

    beta = b.squeeze()[vars]
    chi2 = chi2_test(beta,V,R=R)[0]

    last = np.inf
    p = 0
    i = 0
    Chi2 = []
    while (np.linalg.norm(p-last)>tol) or (i < 30):
        last = p
        if permute_levels is None:
            P= pd.DataFrame(np.random.permutation(X.loc[:,vars]),index=X.index,columns=vars)
        else:
            levels = X.index.names
            fixed = X.index.names.difference(permute_levels)
            P = pd.DataFrame(X.loc[:,vars].unstack(fixed).sample(frac=1).stack(fixed).values,index=X.index,columns=vars)

        myX = pd.concat([X.loc[:,X.columns.difference(vars)],P],axis=1)
        b,V = ols(myX,y)
        Chi2.append(chi2_test(b.squeeze()[vars],V,R=R)[0])
        p = (chi2<Chi2[-1])/(i+1) + last*i/(i+1)
        i += 1
        if VERBOSE: print("Latest chi2 (randomized,actual,p): (%6.2f,%6.2f,%6.4f)" % (Chi2[-1],chi2,p))

    if return_draws:
        return p,pd.Series(Chi2)
    else:
        return p

#+end_src

*** Test of randomization inference
#+begin_src ipython :tangle metrics_miscellany/test/test_randomization_inference.py
import pandas as pd
import scipy.stats.distributions as dists
from metrics_miscellany import estimators, tests
import matplotlib.pyplot as plt

n=1000
p = 0.5
# Generate contextual variables; probability of being female is p
C = pd.DataFrame({'Female':dists.binom(1,p).rvs(size=n)})
C['Male'] = 1-C

delta = pd.Series({"Female":1.,"Male":0.5})

T1 = pd.Series(dists.norm.rvs(size=n),name='Treatment1')
T2 = pd.Series(dists.norm.rvs(size=n),name='Treatment2')

# Interactions:
TC = C.multiply(T1,axis=0)
TC.columns = ['TxFemale','TxMale']

# Construct RHS matrix
X = pd.concat([T1,T2,C,TC],axis=1).iloc[:,:-1]
dC = C@delta

# Generate outcome y with *no* treatment effect, to look for Type I errors
epsilon= pd.Series(dists.norm.rvs(size=n),name='epsilon')

Y = dC + epsilon
Y.name = 'outcome'

p_i = tests.randomization_inference(['Treatment1'],X,Y,VERBOSE=False)

# Generate outcome y with uniform treatment effect, to look for Type II errors
epsilon= pd.Series(dists.norm.rvs(size=n),name='epsilon')

Y = T1 + T2 + dC + epsilon
Y.name = 'outcome'

p_ii = tests.randomization_inference(['Treatment1'],X,Y,VERBOSE=False)

# Test Treatment1 == Treatment2
R = pd.DataFrame({'Coefficients':[1,-1]},index=['Treatment1','Treatment2'])

p_iii = tests.randomization_inference(['Treatment1','Treatment2'],X.drop('TxFemale',axis=1),Y,R=R,VERBOSE=True)

#+end_src

** Maunchy test of sphericity
This test asks whether, given a sample covariance matrix $S$, one can
reject the hypothesis that the population covariance matrix
$\Sigma=\sigma I$; i.e., whether the random vector with variance
matrix $\Sigma$ has a spherical distribution or not (note that the
test is obtained under the assumption that the random vectors are
normally distributed), and is due to Maunchy (1940)[fn:: See
cite:muirhead82 p. 334.].
#+begin_src python :tangle metrics_miscellany/tests.py
import numpy as np
from scipy.stats.distributions import chi2

def maunchy(C,N):
    """Given a sample covariance matrix C estimating using N observations,
       return p-value associated with test of whether the population
       covariance matrix is proportional to the identity matrix.
    """

    raise NotImplementedError

    m = C.shape[0]

    V = np.linalg.det(C)/((np.trace(C)/m)**m)

    rho = 1 - (2*m**2 + m + 2)/(6*m*(N-1))

    w2 = (m-1)*(m-2)*(m+2)*(2*m**3 + 6*m**2 + 3*m + 2)/(288*(m**2) * ((N-1)**2) * rho**2)

    gamma = (((N-1)*rho)**2)*w2

    x2 = -2*(N-1)*rho*np.log(V)  # Chi-squared statistic

    df = (m+2)*(m-1)/2

    px2 = chi2.cdf(x2,df)

    p = px2 + gamma/(((N-1)*rho)**2) * (chi2.cdf(x2,df+4) - px2)

    return x2,1 - px2
#+end_src
**** COMMENT Test of Maunchy
#+begin_src python :tangle metrics_miscellany/test/test_maunchy.py
import numpy as np
from metrics_miscellany import tests
import scipy.stats.distributions as iid
import matplotlib.pyplot as plt

N = 300
k = 10

Chi2 = []
P = []
for m in range(1000):
    X = iid.norm.rvs(size=(N,k))

    C = np.cov(X,rowvar=False)
    x,p = tests.maunchy(C,N)
    Chi2.append(x)
    P.append(p)

df = (k+2)*(k-1)/2


range = np.linspace(np.min(Chi2),np.max(Chi2),500)

fig,ax = plt.subplots()
ax.plot(range,[iid.chi2.pdf(x,df) for x in range])[0]
ax.hist(Chi2,bins=int(np.ceil(np.sqrt(len(Chi2)))),density=True)
#print(
#+end_src
** Inference on eigenvalues
Suppose we wish to test whether a covariance matrix has a structure $\Sigma =
\Lambda\Lambda^\T + \lambda I$, where $\Lambda$ is rank $r$.  This
structure is often assumed in exact factor models, for example.
cite:srivastava-khastri79 (\S 9.5) suggest a simple likelihood ratio test,
implemented here.
#+begin_src python :tangle metrics_miscellany/tests.py
import numpy as np
from scipy.stats.distributions import chi2

def kr79(C,q,N):
    """Given a sample mxm covariance matrix C estimating using N observations,
       return p-value associated with test of whether the population
       covariance matrix has last q eigenvalues equal or not, where q+k=m.
    """

    m = C.shape[0]

    l = np.linalg.eigvalsh(C)  # eigenvalues in *ascending* order

    Q = (np.prod(l[:q])/(np.mean(l[:q])**q))**(N/2) # LR test statistic

    x2 = -2*np.log(Q)  # Chi-squared statistic

    df = (q-1)*(q+2)/2

    px2 = chi2.cdf(x2,df)

    #p = px2 + gamma/(((N-1)*rho)**2) * (chi2.cdf(x2,df+4) - px2)

    return x2,1 - px2
#+end_src
**** Test of kr79
#+begin_src python :tangle metrics_miscellany/test/test_kr79.py
import numpy as np
from metrics_miscellany import tests
import scipy.stats.distributions as iid
from scipy import stats
import matplotlib.pyplot as plt

N = 3000
m = 10
r = 3
q = m - r

# Build covariance matrix for "systematic" variation
Sigma = iid.norm.rvs(size=(m,r))
Sigma = Sigma@Sigma.T   # Positive definite

l,v = np.linalg.eigh(Sigma)
l = np.maximum(l,0)

Ssqrt = v@np.diag(np.sqrt(l))@v

assert np.allclose(Sigma,Ssqrt@Ssqrt.T)

# Build covariance matrix for errors.
## Use (something proportional to) identity for tests of size, otherwise tests of power.
#Psi = 10.0*np.eye(m)  + np.diag(range(1,m+1))/(m**3)
Psi = np.eye(m)
Psi = Psi@Psi.T   # Positive definite

l,v = np.linalg.eigh(Psi)
l = np.maximum(l,0) 

Psisqrt = v@np.diag(np.sqrt(l))@v

assert np.allclose(Psi,Psisqrt@Psisqrt.T)


Chi2 = []
P = []
for s in range(1000):
    X = iid.norm.rvs(size=(N,m))@Ssqrt.T
    e = iid.norm.rvs(size=(N,m))@Psisqrt.T

    C = np.cov(X + e,rowvar=False)
    x,p = tests.kr79(C,q,N)
    Chi2.append(x)
    P.append(p)

df = (q+2)*(q-1)/2

xrange = np.linspace(np.min(Chi2),np.max(Chi2),500)

fig,ax = plt.subplots()
ax.hist(Chi2,bins=int(np.ceil(np.sqrt(len(Chi2)))),density=True)
ax.plot(xrange,[iid.chi2.pdf(x,df) for x in xrange])

assert stats.kstest(P,stats.distributions.uniform.cdf).pvalue>0.01
#+end_src
* Utils
** Some QR Tricks
*** QR Decomposition
Begin with wrapping the numpy qr decomposition to return dataframes.
#+begin_src python  :tangle metrics_miscellany/utils.py
import numpy as np
import pandas as pd

def qr(X):
    """
    Pandas-friendly QR decomposition.
    """
    assert X.shape[0]>=X.shape[1]

    Q,R = np.linalg.qr(X)
    Q = pd.DataFrame(Q,index=X.index, columns=X.columns)
    R = pd.DataFrame(R,index=X.columns, columns=X.columns)

    return Q,R
#+end_src

*** Leverage
Now, use the fact that the leverage of different observations in $X$
are the sums of squares of rows of $Q$ in the $QR$ decomposition
#+begin_src python :tangle metrics_miscellany/utils.py

def leverage(X):
    """
    Return leverage of observations in X (the diagonals of the hat matrix).
    """

    Q = qr(X)[0]

    return (Q**2).sum(axis=1)
#+end_src

*** Hat factory

Now construct a factory that returns a function to put the "hat"
on y.  Though mathematically this looks like $X(X'X)^{-1}X'=QQ'$ in
practice we don't want to construct an $N\times N$ matrix like this,
as it's often too expensive.

#+begin_src python :tangle metrics_miscellany/utils.py
def hat_factory(X):
    """
    Return a function hat(y) that returns X(X'X)^{-1}X'y.

    This is the least squares prediction of y given X.

    We use the fact that  the hat matrix is equal to QQ',
    where Q comes from the QR decomposition of X.
    """
    Q = qr(X)[0]

    def hat(y):
        return Q@(Q.T@y)

    return hat
#+end_src


**  "Fixing" matrices that aren't quite positive definite
#+begin_src python :tangle metrics_miscellany/utils.py
from statsmodels.stats.correlation_tools import cov_nearest as _cov_nearest
import pandas as pd

def cov_nearest(V,threshold=1e-12):
    """
    Return a positive definite matrix which is "nearest" to the symmetric matrix V,
    with the smallest eigenvalue not less than threshold.
    """
    s,U = np.linalg.eigh((V+V.T)/2) # Eigenvalue decomposition of symmetric matrix

    s = np.maximum(s,threshold)

    return V*0 + U@np.diag(s)@U.T  # Trick preserves attributes of dataframe V
#+end_src
** Trimming
#+begin_src python :tangle metrics_miscellany/utils.py
import pandas as pd
import numpy as np

def trim(df,alpha):
    """Trim values below alpha quantile and above (1-alpha) quantile.

    This maps individual extreme elements of df to NaN.
    """
    xmin = df.quantile(alpha)
    xmax = df.quantile(1-alpha)
    return df.where((df>=xmin)*(df<=xmax),np.nan)
#+end_src

* Random
** Random permutations of dataframes
#+begin_src python :tangle metrics_miscellany/random.py
import numpy as np
import pandas as pd

def permutation(df,columns=None,permute_levels=None):
    """Randomly permute rows of df[columns] at permute_levels.
    """

    df = pd.DataFrame(df) # Make sure we have a DataFrame.

    if columns is None: columns = df.columns

    if permute_levels is None:
        P = pd.DataFrame(np.random.permutation(df.loc[:,columns]),index=df.index,columns=columns)
    else:
       fixed = df.index.names.difference(permute_levels)
       P = pd.DataFrame(df.loc[:,columns].unstack(fixed).sample(frac=1).stack(fixed).values,index=df.index,columns=columns)

    return P

#+end_src
*** Test permutation
#+begin_src python :tangle metrics_miscellany/test/test_permutation.py
import numpy as np
import pandas as pd
from metrics_miscellany import random

T = pd.Series(np.random.rand(10)>.5)

df = pd.DataFrame({'a':T,'b':T}).stack()
df = df + 0
df.index.names = ['i','t']

p = random.permutation(df,permute_levels=['i'])

assert np.all(p.unstack('t').std(axis=1)==0)

#+end_src
